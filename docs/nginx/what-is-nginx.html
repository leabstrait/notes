<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Labin Ojha" />
  <title>What Is Nginx</title>
  <style>
    html {
      font-family: Arial, Palatino, Georgia, Times;
      font-size: 11.5pt;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">What Is Nginx</h1>
<p class="author">Labin Ojha</p>
</header>
<h2 id="nginx-usecases">Nginx Usecases</h2>
<ul>
<li>Web Server
<ul>
<li>Serves web content</li>
</ul></li>
<li>Reverse Proxy
<ul>
<li>Load Balancing</li>
<li>Backend Routing</li>
<li>Caching</li>
<li>API Gateway (rate limiting, API versioning, authentication)</li>
</ul></li>
</ul>
<h2 id="nginx-implementation-example">Nginx Implementation Example</h2>
<table>
<colgroup>
<col style="width: 62%" />
<col style="width: 37%" />
</colgroup>
<thead>
<tr class="header">
<th>Before Nginx</th>
<th>After Nginx</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><img src="what-is-nginx/before-nginx.png" alt="Before Nginx" /></td>
<td><img src="what-is-nginx/after-nginx.png" alt="After Nginx" /></td>
</tr>
<tr class="even">
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>- The server can get overloaded as number of connections
increase</td>
<td>Load balanced with Nginx, backend can scale independently</td>
</tr>
<tr class="even">
<td>- We can spin up multiple servers running on several ports but now
the clients have to be aware of them too.</td>
<td>Backend routing with Nginx</td>
</tr>
<tr class="odd">
<td>- The endpoints are not secured, and with multiple servers multiple
certificates need to be issued (or copied to each)</td>
<td>Can issue one certificate with Nginx, multiple certificates not
required</td>
</tr>
<tr class="even">
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><em>Nginx benefits don’t come for free as it is an extra layer and
there is some overhead, that is why Nginx or any reverse proxy has to be
as efficient as possible.</em></p>
<h2 id="nginx-layer-4-vs-layer-7-proxying">Nginx Layer 4 vs Layer 7
proxying</h2>
<ul>
<li>Layer 4/7 refers to OSI model layers.</li>
<li>In Layer 4 we see TCP/IP stack only nothing about the app, we have
access to
<ul>
<li>Source IP, Source Port</li>
<li>Destination IP, Destination Port</li>
<li>Simple packet inspection (SYN/TLS hello)</li>
</ul></li>
<li>In Layer 7 we see the application, HTTP/ gRPC etc..
<ul>
<li>We have access to more context</li>
<li>I know where the client is going, which page they are visiting</li>
<li>Require decryption</li>
</ul></li>
<li>See <a href="../networking/">networking notes</a> for more.</li>
<li>NGINX can operate in Layer 7 (e.g. http) or Layer 4 (tcp)</li>
<li>Layer 4 proxying is useful when NGINX doesn’t understand the
protocol (MySQL database protocol)</li>
<li>Layer 7 proxying is useful when NGINX want to share backend
connections and cache results</li>
<li>Using <code>stream</code> context it becomes a layer 4 proxy</li>
<li>Using <code>http</code> context it becomes a layer 7 proxy</li>
</ul>
<h2 id="tls-termination-and-tls-passthrough">TLS Termination and TLS
Passthrough</h2>
<ul>
<li>TLS stands for Transport Layer Security</li>
<li>It is a way to establish end-to-end encryption between one
another</li>
<li>Symmetric encryption is used for communication (client/server has
the same key)</li>
<li>Asymmetric encryption is used initially to exchange the symmetric
key (diffie hellman)</li>
<li>Encryption alone, whether symmetric or asymmetric, doesn’t guarantee
the identity of communication parties. A middleman can pose as an
authority, so Certificate Authorities (CAs) and verification methods are
crucial for distinguishing between authorized and non-authorized
entities, as encryption’s primary role is content protection, not
identity validation.</li>
<li>See <a href="../networking/">networking notes</a> for more.</li>
</ul>
<h3 id="tls-termination">TLS Termination</h3>
<ul>
<li>Even if NGINX has TLS (e.g. HTTPS) backend may or may not. For
private server environment HTTP is fine if shielded by TLs at Nginx’s
layer. In this case NGINX terminates TLS, decrypts and sends
unencrypted.</li>
<li>If NGINX is TLS and backend is also TLS ( HTTPS ). NGINX terminates
TLS, decrypts, optionally rewrite headers and then re-encrypt the
content to the backend. This introduces latency so the ciphers have to
be fast and performant.</li>
<li>While NGINX can inspect Layer 7 (L7) data, rewrite headers, and
cache content, it either needs to have its own SSL/TLS certificate or
share the certificate used by the backend server. This certificate is
essential for encrypting and decrypting data between NGINX and the
backend.</li>
<li>Suitable for Load Balancing and Content Modification.</li>
<li>Shared Certificates for a Single Domain.</li>
</ul>
<h3 id="tls-passthrough">TLS Passthrough</h3>
<ul>
<li>If the Backend has TLS, NGINX can be used to just to proxy/stream
the packets directly to the backend. In this case Nginx doesn’t respond
to the ‘TLS Hello’ as it is not authorized to terminate TLS. The TLS
handshake is forwarded all the way to the backend just like a tunnel and
back.</li>
<li>There is no caching</li>
<li>There are L4 check only, but more secure as NGINX doesn’t need the
backend certificate.</li>
<li>Preferred for End-to-End Encryption and Enhanced Security.</li>
<li>Appropriate When Content Inspection or Modification Is Not
Required.</li>
<li>One disadvantage is that Nginx cannot share backend connections,
every request will have a new connection and that can be costly.</li>
</ul>
<h2 id="nginx-internal-architecture">Nginx Internal Architecture</h2>
<ul>
<li><p>Nginx has a ‘master process’ that coordinates all other Nginx
processes. It also manages caching, reading it from disk, and refreshing
caches.</p></li>
<li><p>The primary focus is on ‘worker processes’, responsible for most
of the work. Worker processes handle connections and requests.</p>
<figure>
<img src="what-is-nginx/nginx-master-worker-proceses.png"
alt="Nginx Master and Worker Processes" />
<figcaption aria-hidden="true">Nginx Master and Worker
Processes</figcaption>
</figure></li>
<li><p>When Nginx is in ‘auto’ mode, worker processes are spawned based
on the number of hardware threads on the server. Hardware threads can
simulate multiple cores (with <a
href="https://www.intel.com/content/www/us/en/gaming/resources/hyper-threading.html">hyper-threading</a>),
e.g., 4 cores can simulate 8 hardware threads.</p></li>
<li><p><strong>NGINX Threading Architecture</strong></p>
<ul>
<li>When NGINX reverse proxy starts it creates one thread per CPU core
and these worker threads do the heavy lifting. The number of worker
threads are configurable but NGINX recommends one thread per CPU core to
avoid context switching and cache thrashing. In older versions of NGINX
all threads accept connections by competing on the shared listener
socket (by default only one process can listen on IP/port pair). In
recent versions of NGINX this was changed to use socket sharding
(through SO_REUSEPORT socket option) which allows multiple threads to
listen on the same port and the OS will load balance connections on each
accept queue.</li>
</ul>
<table>
<colgroup>
<col style="width: 47%" />
<col style="width: 52%" />
</colgroup>
<thead>
<tr class="header">
<th>Multiple Threads Single Acceptor Architecture</th>
<th>Multiple Threads with Socket Sharding (SO_REUSEPORT)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><img src="what-is-nginx/nginx-multiple-thread-single-acceptor.png"
alt="Nginx Multiple Thread Single Acceptor" /></td>
<td><img
src="what-is-nginx/nginx-multiple-threads-with-socket-sharding.png"
alt="Nginx Multiple Threads with Socket Sharding" /></td>
</tr>
</tbody>
</table>
<p>See more at <a
href="../networking/threads-and-connections-in-the-backend.html">Threads
And Connections In The Backend</a></p></li>
<li><p>When a client establishes a TCP connection to Nginx, connections
are initially placed in a Syn queue and then moved to an accept queue.
The kernel manages the queue but it’s allocated by Nginx.</p></li>
<li><p>Worker processes retrieve connections from the accept queue.
Worker processes are responsible for request handling. Each worker
process is pinned to a CPU core to minimize context switches as Request
handling involves CPU-intensive tasks.</p></li>
<li><p>Some requests are IO bound, requiring reading content from disk,
making upstream network requests or writing to sockets(i.e writing a
response which may also involve encryption). These IO-bound operations
can be slow and cause waits, hence Nginx performs event-driven IO,
allowing the process to perform other tasks during IO wait.</p></li>
<li><p>Nginx scales by adding more worker processes to handle incoming
connections. Each worker process can manage multiple connections
simultaneously. Load balancing distributes connections among worker
processes. The number of worker processes depends on server hardware and
usage patterns.</p></li>
</ul>
<p>See also <a
href="https://www.nginx.com/blog/inside-nginx-how-we-designed-for-performance-scale/">How
Nginx is Designed for Performance and Scale</a></p>
<h2 id="nginx-timeouts">Nginx Timeouts</h2>
<p>In Nginx, timeouts are settings that determine how long the server
should wait for certain events to occur before considering them as
failures. It is critical to ensure efficient use of resources so you
don’t let a runaway process run for a very long time and consume all the
resources and then the client keeps waiting.</p>
<h3 id="frontend-timeouts-when-clients-talk-to-nginx">Frontend Timeouts
(when clients talk to Nginx)</h3>
<ol type="1">
<li><p><strong>client_header_timeout:</strong></p>
<ul>
<li><strong>Description:</strong> <code>client_header_timeout</code>
specifies the maximum time Nginx will wait for a client to send the
request header, including request method, URI, and HTTP headers.</li>
<li><strong>Use Case:</strong> It’s valuable in ensuring that clients
send their request headers within an acceptable time frame, preventing
slow or unresponsive clients from tying up server resources.</li>
<li><strong>Benefit:</strong> Enhances server performance by managing
slow or misbehaving clients effectively and preventing resource
exhaustion.</li>
<li><strong>Status Code:</strong> 408 Request Timeout (if exceeded)</li>
<li><strong>Default Value:</strong> 60s</li>
</ul></li>
<li><p><strong>client_body_timeout:</strong></p>
<ul>
<li><strong>Description:</strong> This timeout determines how long Nginx
waits for the client to send the request body, commonly used for HTTP
POST requests that include data.</li>
<li><strong>Use Case:</strong> In scenarios where clients submit forms,
upload files, or provide data via POST requests,
<code>client_body_timeout</code> ensures timely receipt of this
data.</li>
<li><strong>Benefit:</strong> Prevents connections from remaining open
indefinitely for slow clients, optimizing server resource usage and
responsiveness.</li>
<li><strong>Status Code:</strong> 408 Request Timeout (if exceeded)</li>
<li><strong>Default Value:</strong> 60s</li>
</ul></li>
<li><p><strong>send_timeout:</strong></p>
<ul>
<li><strong>Description:</strong> <code>send_timeout</code> specifies
the maximum time Nginx waits to send data to the client once the
response has started.</li>
<li><strong>Use Case:</strong> In applications like live streaming or
real-time communication, timely data delivery is essential for a smooth
user experience.</li>
<li><strong>Benefit:</strong> Ensures that data is sent to clients
promptly, preventing long delays and potential client-side timeouts,
crucial for real-time applications.</li>
<li><strong>Status Code:</strong> 408 Request Timeout (if exceeded)</li>
<li><strong>Default Value:</strong> 60s</li>
</ul></li>
<li><p><strong>keepalive_timeout:</strong></p>
<ul>
<li><strong>Description:</strong> This timeout defines how long Nginx
keeps a client connection open for potential reuse, allowing multiple
requests over the same connection.</li>
<li><strong>Use Case:</strong> In websites serving multiple assets
(HTML, CSS, JavaScript, images) on a single page, reusing connections
reduces latency and improves load times.</li>
<li><strong>Benefit:</strong> Reduces connection overhead and improves
performance by reusing connections for multiple requests from the same
client.</li>
<li><strong>Status Code:</strong> Not applicable (connection
management)</li>
<li><strong>Default Value:</strong> 75s</li>
</ul></li>
<li><p><strong>lingering_timeout:</strong></p>
<ul>
<li><strong>Description:</strong> <code>lingering_timeout</code>
determines how long Nginx keeps a closed (<a
href="http://nginx.org/en/docs/http/ngx_http_core_module.html#lingering_close">lingering_close</a>)
client connection open to handle late responses. It is used in cases
where Nginx serves cached responses and needs to close the connection
gracefully.</li>
<li><strong>Use Case:</strong> Useful for serving cached content and
ensuring that clients receive responses even if the connection is closed
before the server generates the entire response.</li>
<li><strong>Benefit:</strong> Provides a better user experience by
allowing clients to receive cached content despite a closed connection,
reducing the need for new connections.</li>
<li><strong>Status Code:</strong> Not applicable (connection
management)</li>
<li><strong>Default Value:</strong> 5s</li>
</ul></li>
<li><p><strong>resolver_timeout:</strong></p>
<ul>
<li><strong>Description:</strong> <code>resolver_timeout</code> sets the
maximum time Nginx will wait for DNS resolution when proxying requests
to upstream servers.</li>
<li><strong>Use Case:</strong> When Nginx acts as a reverse proxy and
needs to resolve the DNS of upstream servers, this timeout ensures that
DNS resolution doesn’t cause delays.</li>
<li><strong>Benefit:</strong> Prevents unnecessary delays in request
processing due to slow DNS resolution, ensuring timely delivery of
content.</li>
<li><strong>Status Code:</strong> 408 Request Timeout (if exceeded)</li>
<li><strong>Default Value:</strong> 30s</li>
</ul></li>
</ol>
<h3 id="backend-timeouts-when-nginx-talks-to-backends">Backend Timeouts
(when Nginx talks to backends)</h3>
<ol type="1">
<li><p><strong>proxy_connect_timeout:</strong></p>
<ul>
<li><strong>Description:</strong> <code>proxy_connect_timeout</code>
specifies the maximum time Nginx waits to establish a connection with a
backend server.</li>
<li><strong>Use Case:</strong> In a reverse proxy configuration, it
helps Nginx quickly identify unresponsive backend servers.</li>
<li><strong>Benefit:</strong> Ensures that clients are not left waiting
indefinitely for content from a problematic backend, improving
reliability.</li>
<li><strong>Status Code:</strong> 502 Bad Gateway (if exceeded)</li>
<li><strong>Default Value:</strong> 60s</li>
</ul></li>
<li><p><strong>proxy_send_timeout:</strong></p>
<ul>
<li><strong>Description:</strong> This timeout defines the maximum time
Nginx waits to send data to the backend server after a connection is
established.</li>
<li><strong>Use Case:</strong> In applications like e-commerce
platforms, where requests are forwarded to backend systems for order
processing.</li>
<li><strong>Benefit:</strong> Prevents bottlenecks caused by slow or
unresponsive backend servers, maintaining responsiveness for clients and
preventing request queuing or timeouts.</li>
<li><strong>Status Code:</strong> 502 Bad Gateway (if exceeded)</li>
<li><strong>Default Value:</strong> 60s</li>
</ul></li>
<li><p><strong>proxy_read_timeout:</strong></p>
<ul>
<li><strong>Description:</strong> Specifies the maximum time Nginx waits
for a response from the backend server.</li>
<li><strong>Use Case:</strong> Content-heavy websites fetching data from
backend databases or applications that rely on various backend
services.</li>
<li><strong>Benefit:</strong> Ensures that clients receive timely
responses, preventing unnecessary delays and potential client-side
timeouts due to backend issues.</li>
<li><strong>Status Code:</strong> 504 Gateway Timeout (if exceeded)</li>
<li><strong>Default Value:</strong> 60s</li>
</ul></li>
<li><p><strong>proxy_next_upstream_timeout:</strong></p>
<ul>
<li><strong>Description:</strong>
<code>proxy_next_upstream_timeout</code> sets the maximum time Nginx
waits for a response from the next upstream server in case the current
one fails.</li>
<li><strong>Use Case:</strong> In load-balanced configurations, it
allows Nginx to quickly switch to a backup server if the primary one
fails to respond.</li>
<li><strong>Benefit:</strong> Improves fault tolerance and availability
by minimizing downtime in case of upstream server failures.</li>
<li><strong>Status Code:</strong> 502 Bad Gateway (if exceeded)</li>
<li><strong>Default Value:</strong> 0s</li>
</ul></li>
<li><p><strong>keepalive_timeout:</strong></p>
<ul>
<li><strong>Description:</strong> Already discussed in the frontend
section, <code>keepalive_timeout</code> also plays a role on the backend
for reusing connections between Nginx and upstream servers.</li>
<li><strong>Use Case:</strong> Helps in maintaining efficient and
persistent connections to backend servers, reducing connection
overhead.</li>
<li><strong>Benefit:</strong> Reduces the need to frequently establish
new connections to upstream servers, improving performance and reducing
latency.</li>
<li><strong>Status Code:</strong> Not applicable (connection
management)</li>
<li><strong>Default Value:</strong> 75s</li>
</ul></li>
</ol>
<p>Configuring these timeouts accurately based on your specific use
cases is vital for optimizing the performance, reliability, and
responsiveness of your Nginx web server or reverse proxy while ensuring
efficient resource management and graceful handling of client and
backend server interactions.</p>
<h2 id="nginx-configuration">Nginx Configuration</h2>
<p><strong>Understanding Nginx Configuration Contexts:</strong> - Nginx
configuration is organized into a tree-like structure using curly braces
<code>{}</code> to define contexts. - Contexts group configuration
directives based on their area of concern. - Contexts can be nested,
allowing for inheritance of configurations.</p>
<p><strong>Core Contexts:</strong></p>
<ol type="1">
<li><p><strong>Main Context:</strong></p>
<ul>
<li>The top-level context that affects the entire application.</li>
<li>Configures system user, worker processes, error files, and
more.</li>
</ul>
<p><strong>Example:</strong></p>
<pre class="nginx"><code>user www-data;
worker_processes auto;
error_log /var/log/nginx/error.log;</code></pre></li>
<li><p><strong>Events Context:</strong></p>
<ul>
<li>Nested within the <code>main</code> context.</li>
<li>Sets global options for handling connections.</li>
<li>Controls connection processing techniques.</li>
</ul>
<p><strong>Example:</strong></p>
<pre class="nginx"><code>events {
    worker_connections 1024;
    use epoll;
}</code></pre></li>
<li><p><strong>HTTP Context:</strong></p>
<ul>
<li>Nested within the <code>main</code> context.</li>
<li>The primary context for web server configuration.</li>
<li>Contains directives for handling HTTP or HTTPS connections.</li>
</ul>
<p><strong>Example:</strong></p>
<pre class="nginx"><code>http {
    server_names_hash_bucket_size 64;
    access_log /var/log/nginx/access.log;
}</code></pre></li>
<li><p><strong>Server Context:</strong></p>
<ul>
<li>Nested within the <code>http</code> context.</li>
<li>Defines virtual servers for handling specific requests.</li>
<li>Selects a server based on IP/port and host name.</li>
</ul>
<p><strong>Example:</strong></p>
<pre class="nginx"><code>server {
    listen 80;
    server_name example.com;
    location / {
        # Server configuration here
    }
}</code></pre></li>
<li><p><strong>Location Context:</strong></p>
<ul>
<li>Nested within server contexts.</li>
<li>Used to handle requests based on the request URI.</li>
<li>Contains directives for specific location-based processing.</li>
</ul>
<p><strong>Example:</strong></p>
<pre class="nginx"><code>location /static/ {
    alias /var/www/static/;
}</code></pre></li>
</ol>
<p><strong>Other Contexts (Less Common):</strong> -
<code>split_clients</code>, <code>perl</code>, <code>map</code>,
<code>geo</code>, <code>types</code>, <code>charset_map</code>: These
contexts have specific use cases and are used less frequently.</p>
<p><strong>Upstream Context:</strong> - Used to define and configure
upstream servers for proxying requests. - Enables load balancing when
proxying requests.</p>
<p><strong>Example:</strong>
<code>nginx    upstream backend {        server backend1.example.com;        server backend2.example.com;    }</code></p>
<p><strong>If Context:</strong> - Provides conditional processing of
directives. - Should be used with caution, as it can lead to unexpected
results. - Typically used with <code>return</code> and
<code>rewrite</code> directives.</p>
<p><strong>Example:</strong>
<code>nginx    if ($request_uri ~* "admin") {        return 403;    }</code></p>
<p><strong>Limit_except Context:</strong> - Restricts the use of certain
HTTP methods within a location context. - Useful for controlling access
to specific methods based on client characteristics.</p>
<p><strong>Example:</strong>
<code>nginx    location /restricted-write {        limit_except GET HEAD {            allow 192.168.1.1/24;            deny all;        }    }</code></p>
<p><strong>General Rules for Contexts:</strong> - Apply directives in
the highest context available to avoid repetition. - Utilize multiple
sibling contexts instead of using <code>if</code> logic for processing.
- Favor purpose-made directives over <code>if</code> for better
performance and reliability.</p>
</body>
</html>
